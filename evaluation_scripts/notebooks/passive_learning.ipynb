{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from itertools import accumulate\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "sys.path.append('../../wrappers/')\n",
    "\n",
    "import wrapper_CRF as crf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = \"CONLL2003\"\n",
    "EMBEDDING = \"bert-base-cased_cl4l_pca256\"\n",
    "pretrained_model = \"bert-base-cased\" \n",
    "NCOMP = 256\n",
    "\n",
    "DIR = \"../../expt_results/results_passive/passive_\"+CORPUS+\"_\"+EMBEDDING\n",
    "crfModelPath = DIR+\"/passive_model/passive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "        crfModelPath, \"rb\"\n",
    "    ) as outfile:\n",
    "        model = joblib.load(filename=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempSplit = EMBEDDING.split(\"_\")\n",
    "tembName = tempSplit[1] + \"_\" + tempSplit[0]\n",
    "\n",
    "with open(\"../../saved_embeddings/\"+CORPUS+\"_\"+tembName+\".test\", \"rb\") as outfile:\n",
    "    embeddings = pickle.load(outfile)\n",
    "with open(\"../../datasets/tokenized/\"+CORPUS+\"_test.tags\", \"rb\") as outfile:\n",
    "    tags = json.load(outfile)\n",
    "with open(\"../../datasets/tokenized/\"+CORPUS+\"_test.pos\", \"rb\") as outfile:\n",
    "    pos_tags = json.load(outfile)\n",
    "with open(\"../../datasets/tokenized/\"+CORPUS+\"_test.tokenized\", \"rb\") as outfile:\n",
    "    tknzd_sent = json.load(outfile)\n",
    "\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model, do_basic_tokenize=False\n",
    "    )\n",
    "pretrained_tknzd = [\n",
    "        tokenizer_(\n",
    "            sent,\n",
    "            return_tensors=\"pt\",\n",
    "            is_pretokenized=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for sent in tqdm(tknzd_sent)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in tqdm(range(len(pretrained_tknzd))):\n",
    "    temp_sent = [embeddings[i][0]]\n",
    "    # n = 1\n",
    "    for j in range(1, len(embeddings[i])):\n",
    "        if (\n",
    "            tokenizer_.decode([pretrained_tknzd[i][\"input_ids\"][0][j + 1]])[0]\n",
    "            == \"#\"\n",
    "        ):\n",
    "            temp_sent[-1] = temp_sent[-1] + embeddings[i][j]\n",
    "            # n = n + 1\n",
    "        else:\n",
    "            temp_sent.append(embeddings[i][j])\n",
    "    temp.append(temp_sent[:])\n",
    "\n",
    "    # Truncation for tags and actual tokens, truncation can be done explicitly (rather than seperately)\n",
    "    tknzd_sent[i] = tknzd_sent[i][: len(temp_sent)]\n",
    "    tags[i] = tags[i][: len(temp_sent)]\n",
    "    pos_tags[i] = tags[i][: len(temp_sent)]\n",
    "\n",
    "embeddings = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_r_embeddings(embedding, n_comp=200, seed=29):\n",
    "    embedding_flat = [word.numpy() for sent in embedding for word in sent]\n",
    "    X_ = np.array(embedding_flat)\n",
    "\n",
    "    if np.isinf(X_).any():\n",
    "        print(\"inf: \", X_[np.isinf(X_) == True])\n",
    "\n",
    "    if np.isnan(X_).any():\n",
    "        print(\"nan: \", X_[np.isnan(X_) == True])\n",
    "    \n",
    "    sent_len = [0] + [len(sent) for sent in embedding]\n",
    "    sent_idx = list(accumulate(sent_len))\n",
    "\n",
    "\n",
    "    pca = PCA(n_components=n_comp, random_state=seed)\n",
    "    embeddings = pca.fit_transform(X_)\n",
    "\n",
    "    pca_r_embeddings = [\n",
    "        embeddings[sent_idx[i - 1] : sent_idx[i]]\n",
    "        for i in range(1, len(sent_idx))\n",
    "    ]\n",
    "    # print(\"Variance Explained:\", list(accumulate(pca.explained_variance_ratio_)))\n",
    "\n",
    "    return pca_r_embeddings\n",
    "embeddings_r = pca_r_embeddings(embeddings, n_comp=NCOMP, seed=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(os.path.join(DIR, \"features_config.yaml\"), \"r\") as f:\n",
    "        feature_cfg = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = crf_.sent2features(\n",
    "    feature_cfg,\n",
    "    tknzd_sent,\n",
    "    generator=True,\n",
    "    embeddings=embeddings_r,\n",
    "    pos=pos_tags,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(tags, y_pred)\n",
    "print(classification_report(tags, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
